# IT Purple Hack
#### Кейсы крупнейших IT компаний России, связанные с созданием цифровых продуктов и оптимизацией бизнес-процессов
### ❓ Задача
Среди множества кейсов мы выбрали кейс от **Сбера**: </br>
**Прогнозирование оттока зарплатного клиента ФЛ** </br></br>
Краткое описание:</br>
Клиент получает зарплату на карту банка A. Пока клиент получает зарплату в банке, он считается зарплатным клиентом банка A. В какой-то момент Х он перестает получать зарплату на карту банка A (событие оттока). Необходимо до возникновения события оттока спрогнозировать его, используя данные поведения клиента: транзакции, продукты, мобильное приложение, терминалы, прочее.

### :tada: Результат
:trophy: Место: **3 место**:3rd_place_medal:</br>
**Public score:** </br>
ROC-AUC: **0.77** </br>
F1: **0.13** </br></br>
**Private score:** </br>
ROC-AUC: **0.767** </br>
F1: **0.12** </br></br>

### :page_facing_up: Данные
Для нас организаторы подготовили датасет на ~400к сэмплов на ~1000 признаков. Фичи, конечно же, анонимизированы. Нам предстояла огромная работа над анализом данных) Порядка десятка фичей мы сразу отбросили.

### :memo: Решение
#### Корреляция
В процессе решения мы заметили интересные виды корреляции (не только линейные). Мы предположили, что это один признак - результат аггрегации другого, или, например, один из признаков срез за 1 месяц, а другой - за 3 месяца</br></br>
![image](https://github.com/MALINAYAGODA/Purple_hack/assets/44606552/08c47038-d506-4b99-8308-09cae0b5979b)
</br></br>
#### Отбор признаков
Использовали feature_selection из фреймворка CatBoost, смогли отобрать 70 фичей (~7% от всего количества) без потери качества. Далее поставили полный перебор применения арифметических операций между фичами. Это мы обосновали тем, что какие-то пары фичей имеют какого-то зависимость, какие-то фичи нелинейны. В итоге модель показала существенный (относительно лидерборда) прирост качества.</br></br>
![image](https://github.com/MALINAYAGODA/Purple_hack/assets/44606552/711f0e66-f2ed-480b-9756-d88e65e60cd8)
</br></br>
#### Поиск аномалий
Мы также реализовали поиск аномалий с помощью автоэнкодера. Обучили модель на всех данных, на инференсе брали лосс каждого сэмпла и смотрели: если его значение лосса аномально большое (например чем 95 перцентиль), то этот сэмпл считаем аномальным. 
#### Кластеризация
В какой-то момент мы много пробовали применять эмбеддинги из автоэнкодера. К сожалению их использование не показало никакого прироста качества, однако нам получилось разбить эмбеддинги (в пространстве пониженной размерности) на 3 кластера. Кому я ни показывал - удивила это разбиение. Мы не успели должным образом оценить, дала ли нам прирост качества информация о принадлежности к кластеру, однако, по нашему мнению, этому нужно уделить внимание)</br></br>
![image](https://github.com/MALINAYAGODA/Purple_hack/assets/44606552/0dfa93fe-4408-4732-a747-4bc62c7d6627)
</br></br>
#### Итоговая модель
Нашим решением в итоге является блендинг из 4 CatBoost'ов, их ансабль в итоге показал стабильность на приватном лидерборде. Как нам показалось, по итогу без особой потери качества можно оставить лишь одну модель.
